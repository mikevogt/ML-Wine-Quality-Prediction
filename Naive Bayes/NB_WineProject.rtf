{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset2 Symbol;}}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9\par
The Naive Bayes Algorithm is a Machine Learning method that makes use of conditional probabilites and class independence to quantitatively classify data into certain desired classes, given our prior knowledge. It produces predictions by estimating the probability of the observation belonging to a certain class. Its strengths lie in being an easy to implement and understand algorithm with high prediction accuracies. It further is useful as it is a widely studied algorithm with well-understood shortcomings, providing many resources to learn, apply and improve the model.\par
\par
*The correlation between features is a unitless statistical measure that identifies the relationship between two features. It is similar to the covariance between two features, however the correlation between two features is more informative, providing us with the strength of those relationships as well! We applied the idea of the correlation between features to both the white wine and red wine datasets using the pandas correlation function to calculate all correlation values when comparing all the features with one another, as well as the seaborn library to display a heatmap.*\par
\par

\pard\li720\sa200\sl276\slmult1 For our Naive Bayes model, we classify the quality of white wine and red wine given 11 different features to train the model. We first looked at the data, specifically the features, and deduced that they are all numerical values - leading to our choice of using a Gaussian Distribution in our model.(add why you didnt choose others, add grahps and stuff of data) Since classification can be considered as a key:value mapping from class values to certain data points, we implemented dictionaries as the primary datastructure for storing and manipulating the data. Since a model and datastructure have been chosen, the next step was to build an implement the Naive Bayes model to our data. To achieve this a very easy to understand 3-step appraoch was used, namely:\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 Seperate the data by class value using dictionaries\par
{\pntext\f0 2.\tab}Summarize the dataset by class value(Calculate summary statistics like mean and standard deviation to achieve this)\par
{\pntext\f0 3.\tab}Determine the Class Probalilites of each class using a Gaussian Distribution\par

\pard\sa200\sl276\slmult1 From here, with a strategy in place and with the chosen datastructure and distribution, we implement the model. After implementing the first step of our approach - separate data by class value - we found that the class values we were trying to classify for white wine and red wine are:\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1 red wine keys:  dict_keys([5.0, 6.0, 7.0, 4.0, 8.0, 3.0])\par
{\pntext\f1\'B7\tab}white wine keys:  dict_keys([6.0, 5.0, 7.0, 8.0, 4.0, 3.0, 9.0])\par

\pard\sa200\sl276\slmult1 This matches our evaluation of the data when conducting the exploratory data analysis. Hereafter, with all the data for white and red wine split according to class, as well as being split into training, validation and testing data, we proceed with the second step of our approach - summarizing the data across each of these new classifcation groups for our training data. This will help match a distribution to an associated class value, assisting us in the process of classifying our data.\par
Thus, we move to the final step in our approach - Determining the Class Probalilites of each class using a Gaussian Distribution for our training data. Due to us using real-world data, it fluctuates and can be seen as sporadic. This can make it quite difficult to calculate the probability or determine the likelihood of our given real-world data. To assist in this, the assumption is made that the data must follow some sort of distribution. This assumtion is often made locally, and data manipulation might need to take place in order to remove redundant feautues, outliers, or anything else that might lie in the way of a visible distribution. Often times, for numerical data the best choice is Gaussian, however if your data matches a different distribution, such as Exponential, Naive Bayes may be applied to that distribution. As in the general numerical case, our data primarily matched a Gaussian Distribution. A Gaussian Distribution works very well for numerical data, and can thus be summarized using only the mean and standard deviation.\par
Finally, with everything in place - we can evaluate class probabilites. In this regard, prior to the final Naive Bayes method, a class_probablities() function was implement where it again uses dictionaries as the primary datastructure, mapping the class value to associated class conditional probabilites. This was implemented in the general sense of Naive Bayes, where we first assume class independence in probabilites being calculated seperately for each class. Along with this assumption we then calculate the priors, and multpiply that with the probabilities produced by the Gaussian Distribution for each feature. However, for simplification and implementation purposes we have removed the division for the Naive Bayes Classification as it is redundant in accordance with our end goal. Thus, the output will in fact no longer be strictly the probability of the data belonging to a class, however we still choose the largest value or Maximum a Posteriori. The reason for this implementation is due to the fact that we are more interested in prediction, rather than calculating accurate probabilites. Often, when using real-world data, the conditional probabilites of each class given a feature value may be extremely small. Thereafter, this can result in values too small for python to handle, and this is known as floating point underflow. We do not experience this in our calculations of the class conditional probabilities. This coupled with our analysis of there being no difference in accuracy when using log probabilities led us to removing the implementation of it and rather just using the simplified conditional probability calculation of: \par
\tab\tab *Simplified Naive Bayes Algorithm*\par

\pard\sa200\sl276\slmult1 Therefore, taking all of the above into consideration with our training data, we first use summary statistics to summarize our training data for red and white wine by class.  With this and the calculated class conditional probabilites for the training dataset, we have trained our naive bayes model using the training data.\par

\pard\sa200\sl276\slmult1 Having successfully trained the Naive Bayes model, the next step would be to use validation data to tune hyperparameters! For Naive Bayes, there are however next-to-none. Naive Bayes is so simple and with its independence assumption already achieves a desirable accuracy for predictions.\par
\par
Testing data - explain why white wine is better than red wine\par
With the training of our model complete, and no chosen hyperparamters to tune, we proceeded with testing the model. So, having built the model using the training data and all available features, we ran our testing data through our model too see the types of predictions and the possible accuracies it might return. Our testing data has also been stored using dictionaries. So, for each datapoint that we access in our testing data, along with the summary statistics for our training data , will be passed to our function for calculating the class conditional probabilites associated with with this data point\tab . We then iteratively look for the best class value and associated probability for each of the points, while also appending the "best" chosen class values to our prediction list. So, using all the features and predicting class values in the range 0-10, based of the trainining of our model, we first compared these predictions to our quality label chosen for prediction from our csv.\par
*insert red/white wine quality stuff*\par
Initial results were as expected, achieving accuaries of 54% for red wine and 38% for white wine in data sets the size of 1599 and 4898 respectively. Thus with these initial accuracies, we went back to our data to see how we might improve them. The idea we implemented here, was to decrease the number of classes it has to predicts, while still training it on the class values provided by the quality label\par
So, to attempt our idea, we thought about how, in todays society, many people are not wine connoisseurs and do not require the specific wine qualities ranging from 3.0-9.0. This is because, if someone sees an 8.0, they will believe it to be exceptional. The same is said for 9.0, so this difference by one unit means nothing to the ordinary person in society. For this reason, we chose to split our range of class values into three main groups - Exceptional; Average; Poor. However to implement this grading we had to choose numerical values to represent these:\par
 The class value of '1' was chosen to represent the grouping of our initial class values that are less than 4, relating to 'Poor'.\par
The class value of '2' was chosen to represent the grouping of our initial class values in the range of 4 and 7, relating to 'Average'.\par
The class value of '3' was chose to represent the grouping of our inital class values that were strictly greater than 7, relating to 'Exceptional'.\par
This was completed and stored in our csv as a new class label feature, called 'Quality'. Thus, we used the same trianed model, but converted the prictions to mach our new class label using simple if-statements and ahcieved greatly improved accuracies of 90% for Red Wine and 93% for white wine.\par
This is very interesting! When training our model on the initial class values, and then also predicting using the same range of class values, the red wine had higher accuracies with a much smaller dataset when compared to the white wine with a much larger dataset. The opposite is however true, when we still train our model on the initial class values, but decrease the prediction range of class values. We believe this is due to the the similarity between features, often causing predictions which are slightly incorrect. Thus, the more data you test on, the more you will get incorrect which was proven to be the case with white wine only achieving 38%, while red wine achieved 54%. Then with the decrease in class values, these slight errors werent a problem any more, and now we have the case where the more data results in better predictions which is seen with white wine now achieving 93%, while red wine achieved 90%.\par
}
 